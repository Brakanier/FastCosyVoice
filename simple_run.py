#!/usr/bin/env python3
"""
CosyVoice3 TTS - –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è streaming –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –∑–∞–º–µ—Ä–æ–º –º–µ—Ç—Ä–∏–∫

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ inference_zero_shot –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≥–æ–ª–æ—Å–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç TRT –∏ FP16 –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

–ú–µ—Ç—Ä–∏–∫–∏:
- TTFB (Time To First Byte): –≤—Ä–µ–º—è –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –∞—É–¥–∏–æ
- RTF (Real-Time Factor): –≤—Ä–µ–º—è_—Å–∏–Ω—Ç–µ–∑–∞ / –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å_–∞—É–¥–∏–æ (< 1.0 = –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–ª—Ç–∞–π–º–∞)
- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∞—É–¥–∏–æ
- –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
"""

import sys
import time
import os
import logging
from pathlib import Path

sys.path.append('third_party/Matcha-TTS')

import torch
import torchaudio
from cosyvoice.cli.cosyvoice import CosyVoice3

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è matmul –æ–ø–µ—Ä–∞—Ü–∏–π
torch.set_float32_matmul_precision('high')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é
MODEL_DIR = 'pretrained_models/Fun-CosyVoice3-0.5B'

# –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –∞—É–¥–∏–æ —Ñ–∞–π–ª (3-10 —Å–µ–∫, —á–∏—Å—Ç–∞—è –∑–∞–ø–∏—Å—å)
REFERENCE_AUDIO = 'refs/yaga.wav'

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
OUTPUT_DIR = 'output'

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
INSTRUCTION = "You are a helpful assistant."

# –¢–µ–∫—Å—Ç—ã –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞
SYNTHESIS_TEXTS = [
    "–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ CosyVoice3.",
    "–í—Ç–æ—Ä–æ–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. [cough] [cough] –ë–ª—è—Ç—å! –ù–∞–¥–æ –±—ã –±—Ä–æ—Å–∞—Ç—å –∫—É—Ä–∏—Ç—å",
    "–ò —Ç—Ä–µ—Ç–∏–π —Ç–µ–∫—Å—Ç [laughter] –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ [laughter] –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å [laughter] —Å–º–µ—Ö—É—ë—á–∫–∏.",
]


def load_prompt_text(audio_path: str, instruction: str = INSTRUCTION) -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏–∑ txt —Ñ–∞–π–ª–∞ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç prompt_text.
    
    –§–æ—Ä–º–∞—Ç prompt_text: "{instruction}<|endofprompt|>{—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è}"
    """
    txt_path = audio_path.rsplit('.', 1)[0] + '.txt'
    
    with open(txt_path, 'r', encoding='utf-8') as f:
        transcription = f.read().strip()
    
    return f"{instruction}<|endofprompt|>{transcription}"


def synthesize_streaming(
    cosyvoice: CosyVoice3,
    text: str,
    prompt_text: str,
    spk_id: str,
    sample_rate: int,
    output_path: str
) -> dict:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç streaming —Å–∏–Ω—Ç–µ–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ zero_shot –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.
    
    Args:
        prompt_text: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "{instruction}<|endofprompt|>{—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è}"
    
    Returns:
        dict —Å –∫–ª—é—á–∞–º–∏: ttfb, total_time, audio_duration, rtf, chunk_count
    """
    start_time = time.time()
    first_chunk_time = None
    audio_chunks = []
    chunk_count = 0
    
    for model_output in cosyvoice.inference_zero_shot(
        tts_text=text,
        prompt_text=prompt_text,
        prompt_wav=REFERENCE_AUDIO,
        zero_shot_spk_id=spk_id,
        stream=True,
    ):
        chunk_count += 1
        
        if first_chunk_time is None:
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            first_chunk_time = time.time() - start_time
        
        speech = model_output['tts_speech']
        audio_chunks.append(speech)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    total_time = time.time() - start_time
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    if audio_chunks:
        full_audio = torch.cat(audio_chunks, dim=1)
        torchaudio.save(output_path, full_audio, sample_rate)
        audio_duration = full_audio.shape[1] / sample_rate
    else:
        audio_duration = 0.0
    
    rtf = total_time / audio_duration if audio_duration > 0 else float('inf')
    
    return {
        'ttfb': first_chunk_time or 0.0,
        'total_time': total_time,
        'audio_duration': audio_duration,
        'rtf': rtf,
        'chunk_count': chunk_count,
    }


def main():
    print("=" * 70)
    print("CosyVoice3 TTS - Streaming Inference (zero_shot)")
    print("=" * 70)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(MODEL_DIR):
        logger.error(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_DIR}", exc_info=True)
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ
    if not os.path.exists(REFERENCE_AUDIO):
        logger.error(f"–†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –∞—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω: {REFERENCE_AUDIO}", exc_info=True)
        return
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Path(OUTPUT_DIR).mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º prompt_text –∏–∑ txt —Ñ–∞–π–ª–∞ —Ä—è–¥–æ–º —Å –∞—É–¥–∏–æ
    prompt_text = load_prompt_text(REFERENCE_AUDIO, INSTRUCTION)
    
    print(f"\nüé§ –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –∞—É–¥–∏–æ: {REFERENCE_AUDIO}")
    print(f"üìù –¢–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞: {len(SYNTHESIS_TEXTS)}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (TRT –∏ FP16)
    print("\nüîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    load_start = time.time()
    
    cosyvoice = CosyVoice3(
        model_dir=MODEL_DIR,
        fp16=True,
        load_vllm=False,
        load_trt=True,
    )
    
    load_time = time.time() - load_start
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ dtype
    llm_dtype = next(cosyvoice.model.llm.parameters()).dtype
    flow_dtype = next(cosyvoice.model.flow.parameters()).dtype
    hift_dtype = next(cosyvoice.model.hift.parameters()).dtype
    print(f"üìä LLM dtype: {llm_dtype}, Flow dtype: {flow_dtype}, HiFT dtype: {hift_dtype}")
    
    sample_rate = cosyvoice.sample_rate
    print(f"üìä Sample rate: {sample_rate} Hz")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ø–∏–∫–µ—Ä–∞ (–æ–¥–∏–Ω —Ä–∞–∑)
    print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ø–∏–∫–µ—Ä–∞...")
    spk_id = "reference_speaker"
    embed_start = time.time()
    cosyvoice.add_zero_shot_spk(prompt_text, REFERENCE_AUDIO, spk_id)
    embed_time = time.time() - embed_start
    print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∑–∞ {embed_time:.3f} —Å–µ–∫")
    
    # –°–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º
    all_metrics = []
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
    for idx, text in enumerate(SYNTHESIS_TEXTS, 1):
        print("\n" + "=" * 70)
        print(f"üìÑ –¢–µ–∫—Å—Ç {idx}/{len(SYNTHESIS_TEXTS)}")
        print("=" * 70)
        print(f"üìù {text[:80]}{'...' if len(text) > 80 else ''}")
        
        output_file = os.path.join(OUTPUT_DIR, f'output_{idx:02d}.wav')
        
        try:
            metrics = synthesize_streaming(
                cosyvoice=cosyvoice,
                text=text,
                prompt_text=prompt_text,  # —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ
                spk_id=spk_id,
                sample_rate=sample_rate,
                output_path=output_file,
            )
            
            all_metrics.append(metrics)
            
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")
            print("\nüìä –ú–ï–¢–†–ò–ö–ò:")
            print("-" * 40)
            print(f"‚ö° TTFB:             {metrics['ttfb']:.3f} —Å–µ–∫")
            print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è:      {metrics['total_time']:.3f} —Å–µ–∫")
            print(f"üéµ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:     {metrics['audio_duration']:.3f} —Å–µ–∫")
            print(f"üìà RTF:              {metrics['rtf']:.3f}")
            print(f"üì¶ –ß–∞–Ω–∫–æ–≤:           {metrics['chunk_count']}")
            
            if metrics['rtf'] < 1.0:
                print(f"‚úÖ –ë—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–ª—Ç–∞–π–º–∞ –≤ {1/metrics['rtf']:.1f}x")
            else:
                print(f"‚ö†Ô∏è  –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —Ä–µ–∞–ª—Ç–∞–π–º–∞ –≤ {metrics['rtf']:.1f}x")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ —Ç–µ–∫—Å—Ç–∞ #{idx}: {e}", exc_info=True)
            continue
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞
    if all_metrics:
        print("\n" + "=" * 70)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 70)
        
        avg_ttfb = sum(m['ttfb'] for m in all_metrics) / len(all_metrics)
        avg_rtf = sum(m['rtf'] for m in all_metrics) / len(all_metrics)
        total_audio = sum(m['audio_duration'] for m in all_metrics)
        total_time = sum(m['total_time'] for m in all_metrics)
        
        print(f"–°—Ä–µ–¥–Ω–∏–π TTFB:        {avg_ttfb:.3f} —Å–µ–∫")
        print(f"–°—Ä–µ–¥–Ω–∏–π RTF:         {avg_rtf:.3f}")
        print(f"–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:  {total_audio:.3f} —Å–µ–∫")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è:         {total_time:.3f} —Å–µ–∫")
    
    print("\n" + "=" * 70)
    print("‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 70)
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {OUTPUT_DIR}/")


if __name__ == '__main__':
    main()

